<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Alexander Heinlein</title> <meta name="author" content="Alexander Heinlein"> <meta name="description" content="Personal website of Alexander Heinlein. "> <meta name="keywords" content="numerical analysis, scientific computing, high-performance computing, scientific machine learning"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img//assets/img/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://searhein.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%61.%68%65%69%6E%6C%65%69%6E@%74%75%64%65%6C%66%74.%6E%6C" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0003-1578-8104" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=Pb5ZhSIAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Alexander_Heinlein/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://arxiv.org/a/heinlein_a_1.html" target="_blank" title="arXiv" rel="external nofollow noopener"><i class="ai ai-arxiv"></i></a> <a href="https://github.com/searhein" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/alexander-heinlein-9b569380" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/applied-mathematics/people/dr-a-alexander-heinlein?0%5BL%5D=&amp;cHash=bc6a6a5da90ef761e4b0899ba6d39442" title="Work" rel="external nofollow noopener" target="_blank"><i class="fas fa-briefcase"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/group/">group</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">research</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/research_areas/">research areas</a> <a class="dropdown-item" href="/research_projects/">research projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/software/">software</a> <a class="dropdown-item" href="/repositories/">repositories</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">teaching</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/bachelor_thesis_projects/">bachelor thesis projects</a> <a class="dropdown-item" href="/honors/">honors</a> <a class="dropdown-item" href="/internships/">internships</a> <a class="dropdown-item" href="/master_thesis_projects/">master thesis projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/courses/">courses</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Alexander</span> Heinlein </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic5-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic5-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic5-1400.webp"></source> <img src="/assets/img/prof_pic5.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic5.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>TU Delft</p> <p>DIAM, Faculty of EEMCS</p> <p>Numerical Analysis</p> <p>Mekelweg 4, 2628 CD Delft</p> <p>Room HB 03.290</p> <p style="margin-top: 5px">Tel.: +31 (0)15 27 89135</p> </div> </div> <div class="clearfix"> <p>Alexander Heinlein is assistant professor in the <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/applied-mathematics/numerical-analysis" target="_blank" rel="external nofollow noopener">Numerical Analysis group</a> of the <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/applied-mathematics" target="_blank" rel="external nofollow noopener">Delft Institute of Applied Mathematics (DIAM)</a>, <a href="https://www.tudelft.nl/en/eemcs" target="_blank" rel="external nofollow noopener">Faculty of Electrical Engineering, Mathematics &amp; Computer Science (EEMCS)</a>, at the <a href="https://www.tudelft.nl/en/" target="_blank" rel="external nofollow noopener">Delft University of Technology (TU Delft)</a>. </p> <p>His main research areas are <strong>numerical methods for partial differential equations</strong> and <strong>scientific computing</strong>, in particular, solvers and discretizations based on <strong>domain decomposition and multiscale approaches</strong>. He is interested in <strong>high-performance computing (HPC)</strong> and solving challenging problems involving, e.g., complex geometries, highly heterogeneous coefficient functions, or the coupling of multiple physics. More recently, Alexander also started focusing on the combination of scientific computing and machine learning, a new research area also known as <strong>scientific machine learning (SciML)</strong>. Generally, his work includes the development of new methods and their theoretical foundation as well as their implementation on <strong>current computer architectures (CPUs, GPUs)</strong> and application to real world problems.</p> <div class="social"> <span class="contact-icons text-center"> <a href="mailto:%61.%68%65%69%6E%6C%65%69%6E@%74%75%64%65%6C%66%74.%6E%6C" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0003-1578-8104" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=Pb5ZhSIAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Alexander_Heinlein/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://arxiv.org/a/heinlein_a_1.html" target="_blank" title="arXiv" rel="external nofollow noopener"><i class="ai ai-arxiv"></i></a> <a href="https://github.com/searhein" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/alexander-heinlein-9b569380" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/applied-mathematics/people/dr-a-alexander-heinlein?0%5BL%5D=&amp;cHash=bc6a6a5da90ef761e4b0899ba6d39442" title="Work" rel="external nofollow noopener" target="_blank"><i class="fas fa-briefcase"></i></a> </span> </div> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jan 31, 2026</th> <td> Together with Rongliang Chen, Luca Franco Pavarino, and Xiao-Chuan Cai, I will co-organize the <a href="https://tsimf.tsinghua.edu.cn/info/1549/1752.htm" rel="external nofollow noopener" target="_blank">International Workshop on Numerical and Learning Methods for PDEs</a> at the Tsinghua Sanya International Mathematics Forum (TSIMF) in Sanya, China, February 9-13, 2026. </td> </tr> <tr> <th scope="row">Jan 31, 2026</th> <td> I am looking forward to the <a href="https://www.siam.org/conferences-events/siam-conferences/pp26/" rel="external nofollow noopener" target="_blank">SIAM Conference on Parallel Processing for Scientific Computing (PP26)</a> (March 3–6, 2026, Berlin, Germany) and the <a href="https://hpsf.io/event/hpsf-community-summit-2026/" rel="external nofollow noopener" target="_blank">HPSF Community Summit 2026</a> (February 25–27, 2026, Braunschweig, Germany), where I am serving on the organizing committee. </td> </tr> <tr> <th scope="row">Jan 31, 2026</th> <td> I will present a keynote at the <a href="https://esco2026.fel.zcu.cz/" rel="external nofollow noopener" target="_blank">European Seminar on Computing (ESCO 2026)</a> in Pilsen, Czech Republic, June 1-4, 2026. </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">recent publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CMAME</abbr></div> <div id="Visser:2026:PAC" class="col-sm-8"> <div class="title">PACMANN: Point adaptive collocation method for artificial neural networks</div> <div class="author"> Coen Visser, <em>Alexander Heinlein</em>, and <a href="https://sites.google.com/view/biancagiovanardi/home" rel="external nofollow noopener" target="_blank">Bianca Giovanardi</a> </div> <div class="periodical"> <em>Computer Methods in Applied Mechanics and Engineering</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/https://doi.org/10.1016/j.cma.2025.118723" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">doi</a> <a href="https://www.sciencedirect.com/science/article/pii/S0045782525009958" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a href="https://arxiv.org/abs/2411.19632" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">Preprint</a> </div> <div class="abstract hidden"> <p>Physics-Informed Neural Networks (PINNs) have emerged as a tool for approximating the solution of Partial Differential Equations (PDEs) in both forward and inverse problems. PINNs minimize a loss function which includes the PDE residual determined for a set of collocation points. Previous work has shown that the number and distribution of these collocation points have a significant influence on the accuracy of the PINN solution. Therefore, the effective placement of these collocation points is an active area of research. Specifically, available adaptive collocation point sampling methods have been reported to scale poorly in terms of computational cost when applied to high-dimensional problems. In this work, we address this issue and present the Point Adaptive Collocation Method for Artificial Neural Networks (PACMANN). PACMANN incrementally moves collocation points toward regions of higher residuals using gradient-based optimization algorithms guided by the gradient of the PINN loss function, that is, the squared PDE residual. We apply PACMANN to several forward and inverse problems, including one with a low-regularity solution and 3D Navier Stokes, and demonstrate that this method matches the performance of state-of-the-art methods in terms of the accuracy/efficiency tradeoff for the low-dimensional problems, while outperforming available approaches for high-dimensional problems. Key features of the method include its low computational cost and simplicity of integration into existing physics-informed neural network pipelines. The code is available at https://github.com/CoenVisser/PACMANN.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Visser:2026:PAC</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Visser, Coen and Heinlein, Alexander and Giovanardi, Bianca}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.cma.2025.118723}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Methods in Applied Mechanics and Engineering}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{118723}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PACMANN: Point adaptive collocation method for artificial neural networks}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0045782525009958}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{452}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Schmidt:2026:TGNN" class="col-sm-8"> <div class="title">Mechanistic-driven graph neural network surrogates for pandemic response</div> <div class="author"> Agatha Schmidt, Henrik Zunker, <em>Alexander Heinlein</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Martin J. Kühn' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Scientific Reports</em>, Feb 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1038/s41598-026-39431-5" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">doi</a> <a href="https://www.nature.com/articles/s41598-026-39431-5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a> <a href="https://arxiv.org/abs/2411.06500" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">Preprint</a> </div> <div class="abstract hidden"> <p>During the COVID-19 crisis, mechanistic models have been proven fundamental to guide evidence-based decision making. However, time-critical decisions in a dynamically changing environment restrict the time available for modelers to gather supporting evidence. As infectious disease dynamics are often heterogeneous on a spatial or demographic scale, models should be resolved accordingly. In addition, with a large number of potential interventions, all scenarios can barely be computed on time, even when using supercomputing facilities. We suggest to combine complex mechanistic models with data-driven surrogate models to allow for on-the-fly model adaptations by public health experts. We build upon a spatially and demographically resolved infectious disease model and train a graph neural network for data sets representing early phases of the pandemic. The resulting networks reached an execution time of less than a second, a significant speedup compared to the metapopulation approach. The suggested approach yields potential for on-the-fly execution and, thus, integration of disease dynamics models in low-barrier website applications. For the approach to be used with decision-making, datasets with larger variance will have to be considered.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Schmidt:2026:TGNN</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Schmidt, Agatha and Zunker, Henrik and Heinlein, Alexander and K\"uhn, Martin J.}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1038/s41598-026-39431-5}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Scientific Reports}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6361}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mechanistic-driven graph neural network surrogates for pandemic response}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.nature.com/articles/s41598-026-39431-5}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{16}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Springer LNCSE</abbr></div> <div id="Soliman:2026:SCG" class="col-sm-8"> <div class="title">Sharpened PCG Iteration Bound for High-Contrast Heterogeneous Scalar Elliptic PDEs</div> <div class="author"> Philip Soliman, Filipe Cumaru, and <em>Alexander Heinlein</em> </div> <div class="periodical"> Oct 2026 </div> <div class="periodical"> Accepted for publication in Springer Lecture Notes in Computational Science and Engineering </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2511.12726" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">Preprint</a> </div> <div class="abstract hidden"> <p>We present a new iteration bound for the preconditioned conjugate gradient (PCG) method that accurately captures convergence for systems with clustered eigenspectra, where the classical condition number-based bound is too pessimistic. By using the extremal values of each subcluster in the spectral distribution, our bound is orders of magnitude sharper than the classical bound. We demonstrate its effectiveness on a high-contrast elliptic PDE preconditioned with a two-level overlapping Schwarz method, where it successfully distinguishes between the performance of different (algebraic) preconditioners. A key contribution is theoretical evidence that, for certain high-contrast problems, PCG convergence can make simpler coarse spaces competitive. Conversely, more complex preconditioners are not always necessary. Finally, we show that our bound can be estimated effectively from Ritz values computed during early PCG iterations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@techreport</span><span class="p">{</span><span class="nl">Soliman:2026:SCG</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Soliman, Philip and Cumaru, Filipe and Heinlein, Alexander}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Accepted for publication in Springer Lecture Notes in Computational Science and Engineering}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sharpened PCG Iteration Bound for High-Contrast Heterogeneous Scalar Elliptic PDEs}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Springer LNCSE</abbr></div> <div id="Heinlein:2025:DDA" class="col-sm-8"> <div class="title">Domain decomposition architectures and Gauss–Newton training for physics-informed neural networks</div> <div class="author"> <em>Alexander Heinlein</em>, and Taniya Kapoor</div> <div class="periodical"> Feb 2025 </div> <div class="periodical"> Accepted for publication in the proceedings of the 28th International Conference on Domain Decomposition Methods (DD28) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2510.27018" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">Preprint</a> </div> <div class="abstract hidden"> <p>Approximating the solutions of boundary value problems governed by partial differential equations with neural networks is challenging, largely due to the difficult training process. This difficulty can be partly explained by the spectral bias, that is, the slower convergence of high-frequency components, and can be mitigated by localizing neural networks via (overlapping) domain decomposition. We combine this localization with the Gauss–Newton method as the optimizer to obtain faster convergence than gradient-based schemes such as Adam; this comes at the cost of solving an ill-conditioned linear system in each iteration. Domain decomposition induces a block-sparse structure in the otherwise dense Gauss–Newton system, reducing the computational cost per iteration. Our numerical results indicate that combining localization and Gauss–Newton optimization is promising for neural network-based solvers for partial differential equations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@techreport</span><span class="p">{</span><span class="nl">Heinlein:2025:DDA</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Heinlein, Alexander and Kapoor, Taniya}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Accepted for publication in the proceedings of the 28th International Conference on Domain Decomposition Methods (DD28)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Domain decomposition architectures and Gauss--Newton training for physics-informed neural networks}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ETNA</abbr></div> <div id="Heinlein:2025:MBO" class="col-sm-8"> <div class="title">Monolithic and Block Overlapping Schwarz Preconditioners for the Incompressible Navier–Stokes Equations</div> <div class="author"> <em>Alexander Heinlein</em>, <a href="https://numerik.uni-koeln.de" rel="external nofollow noopener" target="_blank">Axel Klawonn</a>, <a href="https://numerik.uni-koeln.de/arbeitsgruppe/j-knepper-m-sc" rel="external nofollow noopener" target="_blank">Jascha Knepper</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Lea Saßmannshausen' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> Feb 2025 </div> <div class="periodical"> Accepted for publication in Electronic Transactions on Numerical Analysis (ETNA) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2506.16179" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">Preprint</a> </div> <div class="abstract hidden"> <p>Monolithic preconditioners applied to the linear systems arising during the solution of the discretized incompressible Navier–Stokes equations are typically more robust than preconditioners based on incomplete block factorizations. Lower number of iterations and a reduced sensitivity to parameters like velocity and viscosity can significantly outweigh the additional cost for their setup. Different monolithic preconditioning techniques are introduced and compared to a selection of block preconditioners. In particular, two-level additive overlapping Schwarz methods (OSM) are used to set up monolithic preconditioners and to approximate the inverses arising in the block preconditioners. GDSW-type (Generalized Dryja–Smith–Widlund) coarse spaces are used for the second level. These highly scalable, parallel preconditioners have been implemented in the solver framework FROSch (Fast and Robust Overlapping Schwarz), which is part of the software library Trilinos. The new GDSW-type coarse space GDSW* is introduced; combining it with other techniques results in a robust algorithm. The block preconditioners PCD (Pressure Convection–Diffusion), SIMPLE (Semi-Implicit Method for Pressure Linked Equations), and LSC (Least-Squares Commutator) are considered to various degrees. The OSM for the monolithic as well as the block approach allows the optimized combination of different coarse spaces for the velocity and pressure component, enabling the use of tailored coarse spaces. The numerical and parallel performance of the different preconditioning methods for finite element discretizations of stationary as well as time-dependent incompressible fluid flow problems is investigated and compared. Their robustness is analyzed for a range of Reynolds and Courant-Friedrichs-Lewy (CFL) numbers with respect to a realistic problem setting.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@techreport</span><span class="p">{</span><span class="nl">Heinlein:2025:MBO</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Heinlein, Alexander and Klawonn, Axel and Knepper, Jascha and Sa{\ss}mannshausen, Lea}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Accepted for publication in Electronic Transactions on Numerical Analysis (ETNA)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Monolithic and Block Overlapping Schwarz Preconditioners for the Incompressible Navier--Stokes Equations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Wu:2026:DLHIM" class="col-sm-8"> <div class="title">Are Deep Learning Based Hybrid PDE Solvers Reliable? Why Training Paradigms and Update Strategies Matter</div> <div class="author"> Yuhan Wu, Jan Willem Beek, <a href="http://www.victoritadolean.com/" rel="external nofollow noopener" target="_blank">Victorita Dolean</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alexander Heinlein' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> Feb 2026 </div> <div class="periodical"> Submitted </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="" class="btn btn-sm z-depth-0" role="button" target="_blank">Preprint</a> </div> <div class="abstract hidden"> <p>Deep learning-based hybrid iterative methods (DL-HIMs) integrate classical numerical solvers with neural operators, utilizing their complementary spectral biases to accelerate convergence. Despite this promise, many DL-HIMs stagnate at false fixed points where neural updates vanish while the physical residual remains large, raising questions about reliability in scientific computing. In this paper, we provide evidence that performance is highly sensitive to training paradigms and update strategies, even when the neural architecture is fixed. Through a detailed study of a DeepONet-based hybrid iterative numerical transferable solver (HINTS) and an FFT-based Fourier neural solver (FNS), we show that significant physical residuals can persist when training objectives are not aligned with solver dynamics and problem physics. We further examine Anderson acceleration (AA) and demonstrate that its classical form is ill-suited for nonlinear neural operators. To overcome this, we introduce physics-aware Anderson acceleration (PA-AA), which minimizes the physical residual rather than the fixed-point update. Numerical experiments confirm that PA-AA restores reliable convergence in substantially fewer iterations. These findings provide a concrete answer to ongoing controversies surrounding AI-based PDE solvers: reliability hinges not only on architectures but on physically informed training and iteration design.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@techreport</span><span class="p">{</span><span class="nl">Wu:2026:DLHIM</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Yuhan and van Beek, Jan Willem and Dolean, Victorita and Heinlein, Alexander}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Submitted}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Are Deep Learning Based Hybrid {PDE} Solvers Reliable? Why Training Paradigms and Update Strategies Matter}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Propp:2025:DDG" class="col-sm-8"> <div class="title">Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets</div> <div class="author"> Adrienne M. Propp, <a href="https://cfwebprod.sandia.gov/cfdocs/CompResearch/templates/insert/profile.cfm?mperego" rel="external nofollow noopener" target="_blank">Mauro Perego</a>, <a href="https://www.sandia.gov/ccr/staff/eric-christopher-cyr/" rel="external nofollow noopener" target="_blank">Eric C. Cyr</a>, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Anthony Gruber, Amanda Howard, Alexander Heinlein, Panos Stinis, Daniel Tartakovsky' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> Nov 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2512.01888" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">Preprint</a> </div> <div class="abstract hidden"> <p>Accurate yet efficient surrogate models are essential for large-scale simulations of partial differential equations (PDEs), particularly for uncertainty quantification (UQ) tasks that demand hundreds or thousands of evaluations. We develop a physics-inspired graph neural network (GNN) surrogate that operates directly on unstructured meshes and leverages the flexibility of graph attention. To improve both training efficiency and generalization properties of the model, we introduce a domain decomposition (DD) strategy that partitions the mesh into subdomains, trains local GNN surrogates in parallel, and aggregates their predictions. We then employ transfer learning to fine-tune models across subdomains, accelerating training and improving accuracy in data-limited settings. Applied to ice sheet simulations, our approach accurately predicts full-field velocities on high-resolution meshes, substantially reduces training time relative to a single global surrogate model, and provides a ripe foundation for UQ objectives. Our results demonstrate that graph-based DD, combined with transfer learning, provides a scalable and reliable pathway for training GNN surrogates on massive PDE-governed systems, with broad potential for application beyond ice sheet dynamics.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@techreport</span><span class="p">{</span><span class="nl">Propp:2025:DDG</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Propp, Adrienne M. and Perego, Mauro and Cyr, Eric C. and Gruber, Anthony and Howard, Amanda and Heinlein, Alexander and Stinis, Panos and Tartakovsky, Daniel}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Alexander Heinlein. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.Last updated: February 14, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>